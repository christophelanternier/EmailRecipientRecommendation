{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = '../data/'\n",
    "\n",
    "##########################\n",
    "# load files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "training_info = pd.read_csv(path_to_data + 'training_info.csv', sep=',', header=0)\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)\n",
    "test_info = pd.read_csv(path_to_data + 'test_info.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# Correct dates and put datetime format\n",
    "# We do that because we noticed test_set is only composed of email posterior to the ones of train_set. \n",
    "# Datetime format allows to simulate posteriority in our train/test split\n",
    "from datetime import datetime\n",
    "\n",
    "for row in training_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "    if date[:3] == '000':\n",
    "        date = '2' + date[1:]\n",
    "        \n",
    "    training_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43613, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-feb55bcc07ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mquery_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtraining_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-feb55bcc07ff>\u001b[0m in \u001b[0;36mget_sender\u001b[0;34m(query_mid, training)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the sender column in training_info and test_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_sender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 243\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   2873\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2875\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path)\u001b[0m\n\u001b[1;32m   2841\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_possibly_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_extension_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36mis_extension_type\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetimetz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36mis_datetimetz\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m     71\u001b[0m     return ((isinstance(array, ABCDatetimeIndex) and\n\u001b[1;32m     72\u001b[0m              getattr(array, 'tz', None) is not None) or\n\u001b[0;32m---> 73\u001b[0;31m             is_datetime64tz_dtype(array))\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/types/common.pyc\u001b[0m in \u001b[0;36mis_datetime64tz_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeTZDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christophelanternier/anaconda/lib/python2.7/site-packages/pandas/types/dtypes.pyc\u001b[0m in \u001b[0;36mis_dtype\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the sender column in training_info and test_info\n",
    "# !! trÃ¨s long, a faire tourner plus tard et enregistrer les resultats dans un CSV\n",
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "training_info['sender'] = 0\n",
    "for row in training_info.iterrows():\n",
    "    if row[0]%100==0:\n",
    "        print row[0]\n",
    "    query_mid = row[1]['mid']\n",
    "    training_info.loc[row[0], 'sender'] = get_sender(query_mid, training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function \"most_similar\" from doc2vec doesn't work. Impossible to get closest documents. Maybe the corpus isn't big enough to train a neural net. Trying to recover closest docs with cosine similarity doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this cell we prepare labels for emails in case we want to perform a multilabel classification\n",
    "\n",
    "#To see the code that allowed to create that list, see \"archive\"\n",
    "with io.open('../data/person_id.txt') as json_data:\n",
    "    person_id = json.load(json_data)\n",
    "\n",
    "#Create labels for emails\n",
    "labels = []\n",
    "for row in training_info.iterrows():\n",
    "    recipients_id = []\n",
    "    for recipients in row[1]['recipients'].split():\n",
    "        if '@' in recipients:\n",
    "            #print recipients\n",
    "            recipients_id.append(person_id[recipients])\n",
    "            \n",
    "    labels.append(recipients_id)\n",
    "documents = training_info['body'].values\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels_binary = MultiLabelBinarizer().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of documents that fit Doc2Vec expected format. For some reason document have to receive a 'label', \n",
    "#but they are more like tags, don't know if it could be used for classification purposes. \n",
    "documents = []\n",
    "\n",
    "for row in training_info.iterrows():\n",
    "    document = LabeledSentence(words=row[1]['body'].split(), tags=['SENTENCE_'+str(row[0])]) \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for epoch:  0\n",
      "done for epoch:  1\n",
      "done for epoch:  2\n",
      "done for epoch:  3\n",
      "done for epoch:  4\n",
      "done for epoch:  5\n",
      "done for epoch:  6\n",
      "done for epoch:  7\n",
      "done for epoch:  8\n",
      "done for epoch:  9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    random.shuffle(documents)\n",
    "    model.train(documents)\n",
    "    print 'done for epoch: ', str(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('../data/enron.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "#NOTE: we use toarray() for the moment because easier to handle, but less efficient, see later if better is possible\n",
    "array_embedding_sparse = tfidf.fit_transform(training_info['body'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to embed the email body, find its sender, find the 30 closest emails in the embedding space, which were written by the sender, construct a dictionnary of recipients those 30 emails were addressed to, and pick the 10 most frequent reciepient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_similar_sklearn(array_embedding_sparse, query_id):\n",
    "    \n",
    "    similarities = cosine_similarity(array_embedding_sparse, array_embedding_sparse[query_id])\n",
    "    if int(round(sorted(similarities[:,0], reverse=True)[0])) ==1:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1][1:]\n",
    "    else:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1]\n",
    "    \n",
    "    return closest_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "def get_n_closest_emails(sender, n, closest_ids, training, training_info):\n",
    "    # Get all emails' mids from query sender\n",
    "    all_emails_from_sender_mids = [int(k) for k in training[training['sender']==sender]['mids'].values[0].split()]\n",
    "\n",
    "    # Get emails' index from query sender\n",
    "    all_emails_from_sender_ids = training_info[training_info['mid'].isin(all_emails_from_sender_mids)].index.values\n",
    "\n",
    "    # Get the closest emails WRITTEN BY THE SENDER\n",
    "    closest_ids_per_sender = []\n",
    "    for idx in closest_ids:\n",
    "        if idx in all_emails_from_sender_ids:\n",
    "            closest_ids_per_sender.append(idx)\n",
    "        if len(closest_ids_per_sender) == n:\n",
    "            break\n",
    "            \n",
    "    return closest_ids_per_sender\n",
    "\n",
    "def get_10_recipients(closest_ids_per_sender, training_info):\n",
    "    dic_of_recipients = {}\n",
    "    for idx in closest_ids_per_sender:\n",
    "        recipients = training_info.loc[idx,'recipients'].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                if recipient not in dic_of_recipients.keys():\n",
    "                    dic_of_recipients[recipient] = 1\n",
    "                else:\n",
    "                    dic_of_recipients[recipient] += 1\n",
    "\n",
    "    suggested_10_recipients = heapq.nlargest(10, dic_of_recipients, key=dic_of_recipients.get)\n",
    "    \n",
    "    return suggested_10_recipients\n",
    "\n",
    "def mean_ap(suggested_10_recipients, ground_truth):\n",
    "    MAP = 0\n",
    "    correct_guess = 0\n",
    "    for i, suggestion in enumerate(suggested_10_recipients):\n",
    "        if suggestion in ground_truth:\n",
    "            correct_guess +=1\n",
    "            MAP += float(correct_guess)/(i+1)\n",
    "    MAP = float(MAP)/min(10, len(ground_truth))\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_info = training_info.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_info.index = range(training_info.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "#NOTE: we use toarray() for the moment because easier to handle, but less efficient, see later if better is possible\n",
    "array_embedding_sparse = tfidf.fit_transform(training_info['body'].values)\n",
    "#training_info = training_info.sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n"
     ]
    }
   ],
   "source": [
    "query_id = 5\n",
    "all_mean_ap = []\n",
    "\n",
    "\n",
    "for query_id in training_info.tail(1000).index.values:\n",
    "\n",
    "    if query_id%100==0:\n",
    "        print query_id\n",
    "    # number of closest neighbors to collect recipients from:\n",
    "    n = 30\n",
    "\n",
    "    # for testing:\n",
    "    mail = training_info['body'].values[query_id]\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    # ground truth for scoring\n",
    "    ground_truth = training_info['recipients'][query_id].split()\n",
    "\n",
    "    # All closest emails (we will select the ones from the sender later)\n",
    "    closest_ids = most_similar_sklearn2(array_embedding_sparse, mail_tfidf)\n",
    "\n",
    "    query_mid = training_info['mid'][query_id]\n",
    "\n",
    "    # find the sender from the query email\n",
    "    sender = get_sender(query_mid, training)\n",
    "\n",
    "    # find the closest emails to the query one, written by the sender\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, training_info)\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, training_info)\n",
    "\n",
    "    # print\n",
    "    # print suggested_10_recipients\n",
    "    all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39814932697152933"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_mean_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = training_info[training_info.date > datetime(2001, 10, 25)]\n",
    "X_train = training_info[training_info.date <= datetime(2001, 10, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "array_embedding_sparse = tfidf.fit_transform(X_train['body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_sklearn2(array_embedding_sparse, mail_tfidf):\n",
    "    \n",
    "    similarities = cosine_similarity(array_embedding_sparse, mail_tfidf)\n",
    "    if int(round(sorted(similarities[:,0], reverse=True)[0])) ==1:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1][1:]\n",
    "    else:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1]\n",
    "    \n",
    "    return closest_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "all_mean_ap = []\n",
    "\n",
    "#perform tf-idf on X_train\n",
    "X_train.index = range(X_train.shape[0])\n",
    "\n",
    "count = 0\n",
    "query_id = 41055\n",
    "for query_id in X_test.head(1000).index.values:\n",
    "\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "    # number of closest neighbors to collect recipients from:\n",
    "    n = 30\n",
    "    ground_truth = X_test['recipients'][query_id].split()\n",
    "\n",
    "    # for testing\n",
    "    mail = X_test['body'][query_id]\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "\n",
    "    closest_ids = most_similar_sklearn2(array_embedding_sparse, mail_tfidf)\n",
    "    query_mid = X_test['mid'][query_id]\n",
    "\n",
    "    # find the sender from the query email\n",
    "    sender = get_sender(query_mid, training)\n",
    "\n",
    "    # find the closest emails (written by the sender) to the query one\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, X_train)\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, X_train)\n",
    "\n",
    "    #print ground_truth\n",
    "    #print\n",
    "    #print suggested_10_recipients\n",
    "    all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>recipients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>18924</td>\n",
       "      <td>2001-10-31 12:45:25</td>\n",
       "      <td>Mark:Could you (or someone in your group) give...</td>\n",
       "      <td>taylor@enron.com jane.mcbride@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>18959</td>\n",
       "      <td>2001-10-28 07:23:12</td>\n",
       "      <td>Kim and Mark,The attached is a list that Janna...</td>\n",
       "      <td>s..theriot@enron.com taylor@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>18961</td>\n",
       "      <td>2001-10-26 15:29:12</td>\n",
       "      <td>I m going to N.O.  In case you need anything, ...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>18962</td>\n",
       "      <td>2001-10-26 13:18:00</td>\n",
       "      <td>I got this from ICE. Thought you should have a...</td>\n",
       "      <td>taylor@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>19288</td>\n",
       "      <td>2001-11-01 14:27:24</td>\n",
       "      <td>Anne s been working too hard.  She meant Finan...</td>\n",
       "      <td>travis.mccullough@enron.com c..koehler@enron.c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>19289</td>\n",
       "      <td>2001-11-01 12:12:34</td>\n",
       "      <td>Thanks for the list.  Francisco Pinto Leite wi...</td>\n",
       "      <td>c..koehler@enron.com nora.dobin@enron.com l..f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>19290</td>\n",
       "      <td>2001-11-01 11:48:16</td>\n",
       "      <td>-----Original Message-----From: \\t\"Bruning, D...</td>\n",
       "      <td>geoff.storey@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>19291</td>\n",
       "      <td>2001-11-01 11:25:36</td>\n",
       "      <td>=20-----Original Message-----From: Taylor, Mar...</td>\n",
       "      <td>elizabeth.sager@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>19292</td>\n",
       "      <td>2001-10-31 12:52:23</td>\n",
       "      <td>Please switch to Wednesday, Nov. 7</td>\n",
       "      <td>holly.keiser@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>19293</td>\n",
       "      <td>2001-10-31 09:34:59</td>\n",
       "      <td>Can you please give me a call to discuss trave...</td>\n",
       "      <td>mzeleanor@juno.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mid                 date  \\\n",
       "872  18924  2001-10-31 12:45:25   \n",
       "873  18959  2001-10-28 07:23:12   \n",
       "874  18961  2001-10-26 15:29:12   \n",
       "875  18962  2001-10-26 13:18:00   \n",
       "916  19288  2001-11-01 14:27:24   \n",
       "917  19289  2001-11-01 12:12:34   \n",
       "918  19290  2001-11-01 11:48:16   \n",
       "919  19291  2001-11-01 11:25:36   \n",
       "920  19292  2001-10-31 12:52:23   \n",
       "921  19293  2001-10-31 09:34:59   \n",
       "\n",
       "                                                  body  \\\n",
       "872  Mark:Could you (or someone in your group) give...   \n",
       "873  Kim and Mark,The attached is a list that Janna...   \n",
       "874  I m going to N.O.  In case you need anything, ...   \n",
       "875  I got this from ICE. Thought you should have a...   \n",
       "916  Anne s been working too hard.  She meant Finan...   \n",
       "917  Thanks for the list.  Francisco Pinto Leite wi...   \n",
       "918   -----Original Message-----From: \\t\"Bruning, D...   \n",
       "919  =20-----Original Message-----From: Taylor, Mar...   \n",
       "920                Please switch to Wednesday, Nov. 7    \n",
       "921  Can you please give me a call to discuss trave...   \n",
       "\n",
       "                                            recipients  \n",
       "872            taylor@enron.com jane.mcbride@enron.com  \n",
       "873              s..theriot@enron.com taylor@enron.com  \n",
       "874                                   taylor@enron.com  \n",
       "875                                   taylor@enron.com  \n",
       "916  travis.mccullough@enron.com c..koehler@enron.c...  \n",
       "917  c..koehler@enron.com nora.dobin@enron.com l..f...  \n",
       "918                             geoff.storey@enron.com  \n",
       "919                          elizabeth.sager@enron.com  \n",
       "920                             holly.keiser@enron.com  \n",
       "921                                 mzeleanor@juno.com  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.037037037037037035, 0.04, 0.0, 0.0, 0.2, 0.0]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mean_ap[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "for sender, ids in emails_ids_per_sender.iteritems():\n",
    "    recs_temp = []\n",
    "    for my_id in ids:\n",
    "        recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "        recipients = recipients[0].split(' ')\n",
    "        # keep only legitimate email addresses\n",
    "        recipients = [rec for rec in recipients if '@' in rec]\n",
    "        recs_temp.append(recipients)\n",
    "    # flatten    \n",
    "    recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "    # compute recipient counts\n",
    "    rec_occ = dict(Counter(recs_temp))\n",
    "    # order by frequency\n",
    "    sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    # save\n",
    "    address_books[sender] = sorted_rec_occ\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print i\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save all unique recipient names    \n",
    "all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "# save all unique user names \n",
    "all_users = []\n",
    "all_users.extend(all_senders)\n",
    "all_users.extend(all_recs)\n",
    "all_users = list(set(all_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# baselines #                           \n",
    "#############\n",
    "\n",
    "# will contain email ids, predictions for random baseline, and predictions for frequency baseline\n",
    "predictions_per_sender = {}\n",
    "\n",
    "# number of recipients to predict\n",
    "k = 10\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    name_ids = row.tolist()\n",
    "    sender = name_ids[0]\n",
    "    # get IDs of the emails for which recipient prediction is needed\n",
    "    ids_predict = name_ids[1].split(' ')\n",
    "    ids_predict = [int(my_id) for my_id in ids_predict]\n",
    "    random_preds = []\n",
    "    freq_preds = []\n",
    "    # select k most frequent recipients for the user\n",
    "    k_most = [elt[0] for elt in address_books[sender][:k]]\n",
    "    for id_predict in ids_predict:\n",
    "        # select k users at random\n",
    "        random_preds.append(random.sample(all_users, k))\n",
    "        # for the frequency baseline, the predictions are always the same\n",
    "        freq_preds.append(k_most)\n",
    "    predictions_per_sender[sender] = [ids_predict,random_preds,freq_preds]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# write predictions in proper format for Kaggle #                           \n",
    "#################################################\n",
    "\n",
    "path_to_results = '../submission/'\n",
    "\n",
    "with open(path_to_results + 'predictions_random.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        random_preds = preds[1]\n",
    "        for index, my_preds in enumerate(random_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')\n",
    "\n",
    "with open(path_to_results + 'predictions_frequency.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        freq_preds = preds[2]\n",
    "        for index, my_preds in enumerate(freq_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

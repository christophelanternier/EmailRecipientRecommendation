{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "import numpy.random as nprnd\n",
    "\n",
    "stop_words = get_stop_words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = '../data/'\n",
    "\n",
    "##########################\n",
    "# load files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "#training_info = pd.read_csv(path_to_data + 'training_info.csv', sep=',', header=0)\n",
    "training_info = pd.read_csv(path_to_data+\"training_info2.csv\",sep=',', header=0, index_col=0)\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)\n",
    "#test_info = pd.read_csv(path_to_data + 'test_info.csv', sep=',', header=0)\n",
    "test_info = pd.read_csv(path_to_data+\"test_info2.csv\",sep=',', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# Correct dates and put datetime format\n",
    "# We do that because we noticed test_set is only composed of email posterior to the ones of train_set. \n",
    "# Datetime format allows to simulate posteriority in our train/test split\n",
    "from datetime import datetime\n",
    "\n",
    "for row in training_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "    if date[:3] == '000':\n",
    "        date = '2' + date[1:]\n",
    "        \n",
    "    training_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "for row in test_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "        \n",
    "    test_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.007402\n"
     ]
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "dic_recency = {}\n",
    "df_last_sent_emails = X_train_info_sender.sort_values(by = 'date', ascending = False)[:n_recency_features]\n",
    "for row in df_last_sent_emails.iterrows():\n",
    "    recipients = row[1]['recipients'].split()\n",
    "    for recipient in recipients:\n",
    "        if '@' in recipient:\n",
    "            dic_recency[recipient] = dic_recency.get(recipient, 0) + 1\n",
    "norm = sum(dic_recency.values())\n",
    "for k,v in dic_recency.iteritems():\n",
    "    dic_recency[k] = float(v)/norm\n",
    "print datetime.now()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_sklearn(array_embedding_sparse, mail_tfidf, n):\n",
    "    \n",
    "    similarities = cosine_similarity(array_embedding_sparse, mail_tfidf)\n",
    "    if int(round(sorted(similarities[:,0], reverse=True)[0])) ==1:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1][1:]\n",
    "    else:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1]\n",
    "    \n",
    "    return closest_ids[:n], similarities\n",
    "\n",
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "def get_n_closest_emails(sender, n, closest_ids, training, training_info, mail_date, anterior = True):\n",
    "    # The anterior option allows to consider only the closest emails that are anterior to the one considered\n",
    "    \n",
    "    # Get all emails' mids from query sender\n",
    "    #all_emails_from_sender_mids = [int(k) for k in training[training['sender']==sender]['mids'].values[0].split()]\n",
    "\n",
    "    # Get emails' index from query sender\n",
    "    if anterior:\n",
    "        training_info_anterior = training_info[training_info['date'] <= mail_date]\n",
    "    else:\n",
    "        training_info_anterior = training_info\n",
    "        \n",
    "    all_emails_from_sender_ids = training_info_anterior[training_info_anterior['mid'].isin(all_emails_from_sender_mids)].index.values\n",
    "\n",
    "    # Get the closest emails WRITTEN BY THE SENDER\n",
    "    closest_ids_per_sender = []\n",
    "    for idx in closest_ids:\n",
    "        if idx in all_emails_from_sender_ids:\n",
    "            closest_ids_per_sender.append(idx)\n",
    "        if len(closest_ids_per_sender) == n:\n",
    "            break\n",
    "            \n",
    "    return closest_ids_per_sender, all_emails_from_sender_mids\n",
    "\n",
    "def get_10_recipients(closest_ids_per_sender, training_info, similarities):\n",
    "    dic_of_recipients = {}\n",
    "    #weight = len(closest_ids_per_sender)+1\n",
    "    for idx in closest_ids_per_sender:\n",
    "        recipients = training_info.loc[idx,'recipients'].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                dic_of_recipients[recipient] = dic_of_recipients.get(recipient, 0) + similarities[idx][0]\n",
    "\n",
    "    return dic_of_recipients\n",
    "\n",
    "def get_recency_features(X_train_info_sender, mail_date, n_recency_features):    \n",
    "    dic_recency = {}\n",
    "    df_last_sent_emails = X_train_info_sender[X_train_info_sender.date<= mail_date].sort_values(by = 'date', ascending = False)[:n_recency_features]\n",
    "    for row in df_last_sent_emails.iterrows():\n",
    "        recipients = row[1]['recipients'].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                dic_recency[recipient] = dic_recency.get(recipient, 0) + 1\n",
    "    norm = sum(dic_recency.values())\n",
    "    for k,v in dic_recency.iteritems():\n",
    "        dic_recency[k] = float(v)/norm\n",
    "    \n",
    "    return dic_recency\n",
    "\n",
    "def mean_ap(suggested_10_recipients, ground_truth):\n",
    "    MAP = 0\n",
    "    correct_guess = 0\n",
    "    for i, suggestion in enumerate(suggested_10_recipients):\n",
    "        if suggestion in ground_truth:\n",
    "            correct_guess +=1\n",
    "            MAP += float(correct_guess)/(i+1)\n",
    "    MAP = float(MAP)/min(10, len(ground_truth))\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = False\n",
    "training_info = training_info.sort_values(by='date')\n",
    "\n",
    "if submission:\n",
    "    # submission procedure\n",
    "    X_train_info = training_info\n",
    "    X_test_info = test_info\n",
    "    \n",
    "else:\n",
    "    # test procedure\n",
    "    split_date=datetime(2001, 8, 25)\n",
    "    X_train_info = training_info[training_info.date <= split_date]\n",
    "    #X_test_info = training_info[training_info.date > split_date]\n",
    "    \n",
    "    #Randomize selection of test set:\n",
    "    X_test_info = training_info[training_info.date > split_date]\n",
    "    mask = nprnd.choice(range(X_test_info.shape[0]), size=1000, replace=False)\n",
    "    X_test_info.index = range(X_test_info.shape[0])\n",
    "    X_test_info = X_test_info[X_test_info.index.isin(mask)]\n",
    "\n",
    "    X_train_info = training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if submission:\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "    array_embedding_sparse = tfidf.fit_transform(np.concatenate((X_train_info['body'].values,X_test_info['body'].values)))\n",
    "    array_embedding_sparse = array_embedding_sparse[:X_train_info.shape[0]]\n",
    "else:\n",
    "    #With porter stemming:\n",
    "    #tfidf = TfidfVectorizer(tokenizer= tokenize, stop_words = stop_words)\n",
    "    #Without stemming:\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "    array_embedding_sparse = tfidf.fit_transform(X_train_info['body'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('new_features_partial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1 took  0:00:00.002935\n",
      "STEP 2 took  0:00:00.006111\n",
      "STEP 3 took  0:00:00.001176\n",
      "STEP 5 took  0:00:00.008084\n",
      "STEP 7 took  0:00:00.016337\n",
      "STEP 8 took  0:00:01.136626\n",
      "total took: 0:00:01.172883\n"
     ]
    }
   ],
   "source": [
    "all_mean_ap = []\n",
    "all_ground_truth = []\n",
    "all_suggestions = []\n",
    "results = pd.DataFrame(columns=['recipients'])\n",
    "results.index.name = 'mid'\n",
    "\n",
    "index = 0\n",
    "new_features_df = pd.DataFrame(columns=['recipients', 'KNNScore','NSF', 'NRF','recency', 'label'])\n",
    "# number of closest neighbors to collect recipients from:\n",
    "n = 30\n",
    "n_recency_features = 50\n",
    "\n",
    "#re-arrange train index\n",
    "X_train_info.index = range(X_train_info.shape[0])\n",
    "\n",
    "t_all = datetime.now()\n",
    "\n",
    "count = 1\n",
    "query_id = 19516\n",
    "for query_id in [X_train_info.index.values[10000]]:\n",
    "\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "\n",
    "    # STEP 1: preliminary assignment\n",
    "    t1 = datetime.now()\n",
    "    mail = X_train_info['body'][query_id]\n",
    "    mail_date = X_train_info['date'][query_id]\n",
    "    query_mid = X_train_info['mid'][query_id]\n",
    "    ground_truth = X_train_info['recipients'][query_id].split()\n",
    "\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    t_step1 = datetime.now()-t1\n",
    "    print \"STEP 1 took \", t_step1\n",
    "    # END OF STEP 1\n",
    "\n",
    "    # STEP 2: get sender\n",
    "    t2 = datetime.now()\n",
    "    sender = X_train_info[X_train_info.mid == query_mid]['sender'].values[0]\n",
    "    X_train_info_sender = X_train_info[X_train_info.sender == sender]\n",
    "    index_sender = X_train_info_sender.index.values\n",
    "    X_train_info_sender.index = range(X_train_info_sender.shape[0])\n",
    "    array_embedding_sparse_sender = array_embedding_sparse[index_sender]\n",
    "    t_step2 = datetime.now() - t2\n",
    "    print \"STEP 2 took \", t_step2\n",
    "    # END OF STEP 2 --- > divided by 100\n",
    "\n",
    "    # STEP 3: Rank mails by cosine similarity with searched mail\n",
    "    t3 = datetime.now()\n",
    "    closest_ids_per_sender, similarities = most_similar_sklearn(array_embedding_sparse_sender, mail_tfidf, n)\n",
    "    t_step3 = datetime.now() - t3\n",
    "    print \"STEP 3 took \", t_step3\n",
    "    # END OF STEP 3\n",
    "\n",
    "    # STEP 5: compute recency features\n",
    "    t5 = datetime.now()\n",
    "    dic_recency = get_recency_features(X_train_info_sender, mail_date, n_recency_features)\n",
    "    t_step5 = datetime.now() - t5\n",
    "    print \"STEP 5 took \", t_step5\n",
    "    # END OF STEP 5\n",
    "\n",
    "    # STEP 7: Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    t7 = datetime.now()\n",
    "    dic_of_recipients = get_10_recipients(closest_ids_per_sender, X_train_info_sender, similarities)\n",
    "    t_step7 = datetime.now() - t7\n",
    "    print \"STEP 7 took \", t_step7\n",
    "    # END OF STEP 7\n",
    "\n",
    "    # STEP 8: Create new features\n",
    "    t8 = datetime.now()\n",
    "    for k,v in dic_of_recipients.iteritems():\n",
    "        KNNScore = v\n",
    "        NSF = sent_to[sender][k]\n",
    "        if sender in received_from.keys():\n",
    "            if k in received_from[sender].keys():\n",
    "                NRF = received_from[sender][k]\n",
    "            else:\n",
    "                NRF = 0\n",
    "        else:\n",
    "            NRF = 0\n",
    "        if k in ground_truth:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        if k in dic_recency.keys():\n",
    "            recency = dic_recency[k]\n",
    "        else:\n",
    "            recency = 0\n",
    "        new_features_df.loc[index] = [k, KNNScore, NSF, NRF, recency, label]\n",
    "        index +=1\n",
    "\n",
    "    t_step8 = datetime.now() - t8\n",
    "    print \"STEP 8 took \", t_step8\n",
    "    # END OF STEP 8\n",
    "\n",
    "print \"total took:\", datetime.now()-t_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic_of_recipients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(closest_ids_per_sender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get NSF and NRF\n",
    "- NSF: number of messages sent by sender to this recipient, divided by all messages sent by sender \n",
    "- NRF: number of messages received from the recipient to the sender, divided by all messages received by sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43613, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in training_info.iterrows():\n",
    "    recipients = row[1]['recipients'].split()\n",
    "    for recipient in recipients:\n",
    "        if recipient == 'wsmith@wordsmith.org':\n",
    "            print recipient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wsmith@wordsmith.org'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6ea63c9af469>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreceived_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wsmith@wordsmith.org'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'wsmith@wordsmith.org'"
     ]
    }
   ],
   "source": [
    "received_from['wsmith@wordsmith.org']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_to = {}\n",
    "for row in training.iterrows():\n",
    "    sender = row[1]['sender']\n",
    "    sent_to[sender] = {}\n",
    "    for mid in row[1]['mids'].split():\n",
    "        recipients = training_info[training_info.mid == int(mid)]['recipients'].values[0].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                if recipient in sent_to[sender].keys():\n",
    "                    sent_to[sender][recipient] += 1\n",
    "                else:\n",
    "                    sent_to[sender][recipient] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "received_from = {}\n",
    "for sender in sent_to.keys():\n",
    "    for recipient in sent_to[sender].keys():\n",
    "        if recipient not in received_from.keys():\n",
    "            received_from[recipient] = {}\n",
    "        if sender not in received_from[recipient].keys():\n",
    "            received_from[recipient][sender] = sent_to[sender][recipient]\n",
    "        else:\n",
    "            received_from[recipient][sender] += sent_to[sender][recipient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalization:\n",
    "for sender in sent_to.keys():\n",
    "    norm = sum(sent_to[sender].values())\n",
    "    for recipient in sent_to[sender].keys():\n",
    "        sent_to[sender][recipient] = float(sent_to[sender][recipient])/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalization:\n",
    "for recipient in received_from.keys():\n",
    "    norm = sum(received_from[recipient].values())\n",
    "    for sender in received_from[recipient].keys():\n",
    "        received_from[recipient][sender] = float(received_from[recipient][sender])/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

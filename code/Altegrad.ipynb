{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import io\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = '../data/'\n",
    "\n",
    "##########################\n",
    "# load some of the files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "training_info = pd.read_csv(path_to_data + 'training_info.csv', sep=',', header=0)\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)\n",
    "test_info = pd.read_csv(path_to_data + 'test_info.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correct dates and put datetime format\n",
    "# We do that because we noticed test_set is only composed of email posterior to the ones of train_set. \n",
    "# Datetime format allows to simulate posteriority in our train/test split\n",
    "from datetime import datetime\n",
    "\n",
    "for row in training_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "    if date[:3] == '000':\n",
    "        date = '2' + date[1:]\n",
    "        \n",
    "    training_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function \"most_similar\" from doc2vec doesn't work. Impossible to get closest documents. Maybe the corpus isn't big enough to train a neural net. Trying to recover closest docs with cosine similarity doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this cell we prepare labels for emails in case we want to perform a multilabel classification\n",
    "\n",
    "#To see the code that allowed to create that list, see \"archive\"\n",
    "with io.open('../data/person_id.txt') as json_data:\n",
    "    person_id = json.load(json_data)\n",
    "\n",
    "#Create labels for emails\n",
    "labels = []\n",
    "for row in training_info.iterrows():\n",
    "    recipients_id = []\n",
    "    for recipients in row[1]['recipients'].split():\n",
    "        if '@' in recipients:\n",
    "            #print recipients\n",
    "            recipients_id.append(person_id[recipients])\n",
    "            \n",
    "    labels.append(recipients_id)\n",
    "documents = training_info['body'].values\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels_binary = MultiLabelBinarizer().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of documents that fit Doc2Vec expected format. For some reason document have to receive a 'label', \n",
    "#but they are more like tags, don't know if it could be used for classification purposes. \n",
    "documents = []\n",
    "\n",
    "for row in training_info.iterrows():\n",
    "    document = LabeledSentence(words=row[1]['body'].split(), tags=['SENTENCE_'+str(row[0])]) \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for epoch:  0\n",
      "done for epoch:  1\n",
      "done for epoch:  2\n",
      "done for epoch:  3\n",
      "done for epoch:  4\n",
      "done for epoch:  5\n",
      "done for epoch:  6\n",
      "done for epoch:  7\n",
      "done for epoch:  8\n",
      "done for epoch:  9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    random.shuffle(documents)\n",
    "    model.train(documents)\n",
    "    print 'done for epoch: ', str(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('../data/enron.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "#NOTE: we use toarray() for the moment because easier to handle, but less efficient, see later if better is possible\n",
    "array_embedding_sparse = tfidf.fit_transform(training_info['body'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to embed the email body, find its sender, find the 30 closest emails in the embedding space, which were written by the sender, construct a dictionnary of recipients those 30 emails were addressed to, and pick the 10 most frequent reciepient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_n_similar_sklearn(array_embedding_sparse, query_id):\n",
    "    \n",
    "    similarities = cosine_similarity(array_embedding_sparse, array_embedding_sparse[query_id])\n",
    "    if int(round(sorted(similarities[:,0], reverse=True)[0])) ==1:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1][1:]\n",
    "    else:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1]\n",
    "    \n",
    "    return closest_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "def get_n_closest_emails(sender, n, closest_ids, training, training_info):\n",
    "    # Get all emails' mids from query sender\n",
    "    all_emails_from_sender_mids = [int(k) for k in training[training['sender']==sender]['mids'].values[0].split()]\n",
    "\n",
    "    # Get emails' index from query sender\n",
    "    all_emails_from_sender_ids = training_info[training_info['mid'].isin(all_emails_from_sender_mids)].index.values\n",
    "\n",
    "    # Get the closest emails WRITTEN BY THE SENDER\n",
    "    closest_ids_per_sender = []\n",
    "    for idx in closest_ids:\n",
    "        if idx in all_emails_from_sender_ids:\n",
    "            closest_ids_per_sender.append(idx)\n",
    "        if len(closest_ids_per_sender) == n:\n",
    "            break\n",
    "            \n",
    "    return closest_ids_per_sender\n",
    "\n",
    "def get_10_recipients(closest_ids_per_sender, training_info):\n",
    "    dic_of_recipients = {}\n",
    "    for idx in closest_ids_per_sender:\n",
    "        recipients = training_info.loc[idx,'recipients'].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                if recipient not in dic_of_recipients.keys():\n",
    "                    dic_of_recipients[recipient] = 1\n",
    "                else:\n",
    "                    dic_of_recipients[recipient] += 1\n",
    "\n",
    "    suggested_10_recipients = heapq.nlargest(10, dic_of_recipients, key=dic_of_recipients.get)\n",
    "    \n",
    "    return suggested_10_recipients\n",
    "\n",
    "def mean_ap(suggested_10_recipients, ground_truth):\n",
    "    MAP = 0\n",
    "    correct_guess = 0\n",
    "    for i, suggestion in enumerate(suggested_10_recipients):\n",
    "        if suggestion in ground_truth:\n",
    "            correct_guess +=1\n",
    "            MAP += float(correct_guess)/(i+1)\n",
    "    MAP = float(MAP)/min(10, len(ground_truth))\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['portland.desk@enron.com']\n",
      "\n",
      "['portland.desk@enron.com', 'bill.williams@enron.com', 'chris.stokley@enron.com', 'kathy.axford@enron.com', 'holden.salisbury@enron.com', 'kate.symes@enron.com', 'm..driscoll@enron.com', 'c..bland@enron.com', 'f..calger@enron.com', 'diana.scholtes@enron.com']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "query_id = 6\n",
    "all_mean_ap = []\n",
    "\n",
    "for query_id in training_info.head(1000).index.values:\n",
    "    \n",
    "    if query_id%100==0:\n",
    "        print query_id\n",
    "    # number of closest neighbors to collect recipients from:\n",
    "    n = 30\n",
    "    ground_truth = training_info['recipients'][query_id].split()\n",
    "\n",
    "    closest_ids = most_n_similar_sklearn(array_embedding_sparse, query_id)\n",
    "    query_mid = training_info['mid'][query_id]\n",
    "\n",
    "    # find the sender from the query email\n",
    "    sender = get_sender(query_mid, training)\n",
    "\n",
    "    # find the closest emails to the query one, written by the sender\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, training_info)\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, training_info)\n",
    "\n",
    "    # print\n",
    "    # print suggested_10_recipients\n",
    "    all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = training_info[training_info.date > datetime(2001, 10, 15)]\n",
    "X_train = training_info[training_info.date <= datetime(2001, 10, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mid</th>\n",
       "      <th>date</th>\n",
       "      <th>body</th>\n",
       "      <th>recipients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1189</td>\n",
       "      <td>2001-11-19 08:16:20</td>\n",
       "      <td>The following expense report is ready for appr...</td>\n",
       "      <td>kevin.hyatt@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2295</td>\n",
       "      <td>2001-10-23 14:37:36</td>\n",
       "      <td>State Regulations Forces Questar to Cancel Par...</td>\n",
       "      <td>mark.mcconnell@enron.com martin.gonzalez@enron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>5219</td>\n",
       "      <td>2001-10-15 08:58:07</td>\n",
       "      <td>Attached is a NON BINDING letter of interest t...</td>\n",
       "      <td>james.centilli@enron.com mansoor.abdmoulaie@en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>5480</td>\n",
       "      <td>2001-10-29 12:20:59</td>\n",
       "      <td>Tracy:    Since I believe that Rod will also b...</td>\n",
       "      <td>tracy.geaccone@enron.com rod.hayslett@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5536</td>\n",
       "      <td>2001-10-18 07:17:26</td>\n",
       "      <td>Good morning, Tracy!   Stan wants us to look a...</td>\n",
       "      <td>tracy.geaccone@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>7014</td>\n",
       "      <td>2001-10-22 16:10:01</td>\n",
       "      <td>Please look at the attached.  If there are som...</td>\n",
       "      <td>stephen.dowd@enron.com bob.chandler@enron.com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7422</td>\n",
       "      <td>2001-10-29 11:33:28</td>\n",
       "      <td>Richard: I understand from Elliot that we want...</td>\n",
       "      <td>richard.ring@enron.com elliot.mainzer@enron.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7476</td>\n",
       "      <td>2001-10-29 11:33:28</td>\n",
       "      <td>Richard: I understand from Elliot that we want...</td>\n",
       "      <td>richard.ring@enron.com elliot.mainzer@enron.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>7722</td>\n",
       "      <td>2001-10-15 11:18:38</td>\n",
       "      <td>We are hoping to have the retreat on Thursday,...</td>\n",
       "      <td>jeff.shields@enron.com jesse.bryson@enron.com ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>7723</td>\n",
       "      <td>2001-10-16 10:57:44</td>\n",
       "      <td>Richard: I suggest you get in on the evening o...</td>\n",
       "      <td>richard.ring@enron.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mid                 date  \\\n",
       "31   1189  2001-11-19 08:16:20   \n",
       "40   2295  2001-10-23 14:37:36   \n",
       "118  5219  2001-10-15 08:58:07   \n",
       "119  5480  2001-10-29 12:20:59   \n",
       "120  5536  2001-10-18 07:17:26   \n",
       "133  7014  2001-10-22 16:10:01   \n",
       "134  7422  2001-10-29 11:33:28   \n",
       "135  7476  2001-10-29 11:33:28   \n",
       "142  7722  2001-10-15 11:18:38   \n",
       "143  7723  2001-10-16 10:57:44   \n",
       "\n",
       "                                                  body  \\\n",
       "31   The following expense report is ready for appr...   \n",
       "40   State Regulations Forces Questar to Cancel Par...   \n",
       "118  Attached is a NON BINDING letter of interest t...   \n",
       "119  Tracy:    Since I believe that Rod will also b...   \n",
       "120  Good morning, Tracy!   Stan wants us to look a...   \n",
       "133  Please look at the attached.  If there are som...   \n",
       "134  Richard: I understand from Elliot that we want...   \n",
       "135  Richard: I understand from Elliot that we want...   \n",
       "142  We are hoping to have the retreat on Thursday,...   \n",
       "143  Richard: I suggest you get in on the evening o...   \n",
       "\n",
       "                                            recipients  \n",
       "31                               kevin.hyatt@enron.com  \n",
       "40   mark.mcconnell@enron.com martin.gonzalez@enron...  \n",
       "118  james.centilli@enron.com mansoor.abdmoulaie@en...  \n",
       "119    tracy.geaccone@enron.com rod.hayslett@enron.com  \n",
       "120                           tracy.geaccone@enron.com  \n",
       "133  stephen.dowd@enron.com bob.chandler@enron.com ...  \n",
       "134  richard.ring@enron.com elliot.mainzer@enron.co...  \n",
       "135  richard.ring@enron.com elliot.mainzer@enron.co...  \n",
       "142  jeff.shields@enron.com jesse.bryson@enron.com ...  \n",
       "143                             richard.ring@enron.com  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "array_embedding_sparse = tfidf.fit_transform(X_train['body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "all_mean_ap = []\n",
    "\n",
    "#perform tf-idf on X_train\n",
    "\n",
    "count = 0\n",
    "for query_id in X_test.head(2000).index.values:\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "    # number of closest neighbors to collect recipients from:\n",
    "    n = 30\n",
    "    ground_truth = training_info['recipients'][query_id].split()\n",
    "\n",
    "    closest_ids = most_n_similar_sklearn(array_embedding_sparse, query_id)\n",
    "    query_mid = X_test['mid'][query_id]\n",
    "\n",
    "    # find the sender from the query email\n",
    "    sender = get_sender(query_mid, training)\n",
    "\n",
    "    # find the closest emails (written by the sender) to the query one\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, X_train)\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, X_train)\n",
    "\n",
    "    #print ground_truth\n",
    "    #print\n",
    "    #print suggested_10_recipients\n",
    "    all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22607515384857646"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_mean_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "for sender, ids in emails_ids_per_sender.iteritems():\n",
    "    recs_temp = []\n",
    "    for my_id in ids:\n",
    "        recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "        recipients = recipients[0].split(' ')\n",
    "        # keep only legitimate email addresses\n",
    "        recipients = [rec for rec in recipients if '@' in rec]\n",
    "        recs_temp.append(recipients)\n",
    "    # flatten    \n",
    "    recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "    # compute recipient counts\n",
    "    rec_occ = dict(Counter(recs_temp))\n",
    "    # order by frequency\n",
    "    sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    # save\n",
    "    address_books[sender] = sorted_rec_occ\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print i\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save all unique recipient names    \n",
    "all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "# save all unique user names \n",
    "all_users = []\n",
    "all_users.extend(all_senders)\n",
    "all_users.extend(all_recs)\n",
    "all_users = list(set(all_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# baselines #                           \n",
    "#############\n",
    "\n",
    "# will contain email ids, predictions for random baseline, and predictions for frequency baseline\n",
    "predictions_per_sender = {}\n",
    "\n",
    "# number of recipients to predict\n",
    "k = 10\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    name_ids = row.tolist()\n",
    "    sender = name_ids[0]\n",
    "    # get IDs of the emails for which recipient prediction is needed\n",
    "    ids_predict = name_ids[1].split(' ')\n",
    "    ids_predict = [int(my_id) for my_id in ids_predict]\n",
    "    random_preds = []\n",
    "    freq_preds = []\n",
    "    # select k most frequent recipients for the user\n",
    "    k_most = [elt[0] for elt in address_books[sender][:k]]\n",
    "    for id_predict in ids_predict:\n",
    "        # select k users at random\n",
    "        random_preds.append(random.sample(all_users, k))\n",
    "        # for the frequency baseline, the predictions are always the same\n",
    "        freq_preds.append(k_most)\n",
    "    predictions_per_sender[sender] = [ids_predict,random_preds,freq_preds]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# write predictions in proper format for Kaggle #                           \n",
    "#################################################\n",
    "\n",
    "path_to_results = '../submission/'\n",
    "\n",
    "with open(path_to_results + 'predictions_random.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        random_preds = preds[1]\n",
    "        for index, my_preds in enumerate(random_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')\n",
    "\n",
    "with open(path_to_results + 'predictions_frequency.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        freq_preds = preds[2]\n",
    "        for index, my_preds in enumerate(freq_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

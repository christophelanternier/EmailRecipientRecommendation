{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import numpy as np\n",
    "import heapq\n",
    "import json\n",
    "import operator\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy.random as nprnd\n",
    "#nltk.download()\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "from gensim.models.doc2vec import LabeledSentence, Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_data = '../data/'\n",
    "\n",
    "##########################\n",
    "# load files #                           \n",
    "##########################\n",
    "\n",
    "training = pd.read_csv(path_to_data + 'training_set.csv', sep=',', header=0)\n",
    "training_info = pd.read_csv(path_to_data + 'training_info.csv', sep=',', header=0)\n",
    "#training_info = pd.read_csv(path_to_data+\"training_info2.csv\",sep=',', header=0, index_col=0)\n",
    "test = pd.read_csv(path_to_data + 'test_set.csv', sep=',', header=0)\n",
    "test_info = pd.read_csv(path_to_data + 'test_info.csv', sep=',', header=0)\n",
    "#test_info = pd.read_csv(path_to_data+\"test_info2.csv\",sep=',', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "# Correct dates and put datetime format\n",
    "# We do that because we noticed test_set is only composed of email posterior to the ones of train_set. \n",
    "# Datetime format allows to simulate posteriority in our train/test split\n",
    "from datetime import datetime\n",
    "\n",
    "for row in training_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "    if date[:3] == '000':\n",
    "        date = '2' + date[1:]\n",
    "        \n",
    "    training_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "for row in test_info.sort(['date']).iterrows():\n",
    "    date = row[1]['date']\n",
    "        \n",
    "    test_info.loc[row[0], 'date'] = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n"
     ]
    }
   ],
   "source": [
    "# Get the sender column in training_info and test_info\n",
    "# !! tr√®s long, a faire tourner plus tard et enregistrer les resultats dans un CSV\n",
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "test_info['sender'] = 0\n",
    "for row in test_info.iterrows():\n",
    "    if row[0]%100==0:\n",
    "        print row[0]\n",
    "    query_mid = row[1]['mid']\n",
    "    test_info.loc[row[0], 'sender'] = get_sender(query_mid, test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function \"most_similar\" from doc2vec doesn't work. Impossible to get closest documents. Maybe the corpus isn't big enough to train a neural net. Trying to recover closest docs with cosine similarity doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this cell we prepare labels for emails in case we want to perform a multilabel classification\n",
    "\n",
    "#To see the code that allowed to create that list, see \"archive\"\n",
    "with io.open('../data/person_id.txt') as json_data:\n",
    "    person_id = json.load(json_data)\n",
    "\n",
    "#Create labels for emails\n",
    "labels = []\n",
    "for row in training_info.iterrows():\n",
    "    recipients_id = []\n",
    "    for recipients in row[1]['recipients'].split():\n",
    "        if '@' in recipients:\n",
    "            #print recipients\n",
    "            recipients_id.append(person_id[recipients])\n",
    "            \n",
    "    labels.append(recipients_id)\n",
    "documents = training_info['body'].values\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "labels_binary = MultiLabelBinarizer().fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a list of documents that fit Doc2Vec expected format. For some reason document have to receive a 'label', \n",
    "#but they are more like tags, don't know if it could be used for classification purposes. \n",
    "documents = []\n",
    "\n",
    "for row in training_info.iterrows():\n",
    "    document = LabeledSentence(words=row[1]['body'].split(), tags=['SENTENCE_'+str(row[0])]) \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done for epoch:  0\n",
      "done for epoch:  1\n",
      "done for epoch:  2\n",
      "done for epoch:  3\n",
      "done for epoch:  4\n",
      "done for epoch:  5\n",
      "done for epoch:  6\n",
      "done for epoch:  7\n",
      "done for epoch:  8\n",
      "done for epoch:  9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    random.shuffle(documents)\n",
    "    model.train(documents)\n",
    "    print 'done for epoch: ', str(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('../data/enron.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to embed the email body, find its sender, find the 30 closest emails in the embedding space, which were written by the sender, construct a dictionnary of recipients those 30 emails were addressed to, and pick the 10 most frequent reciepient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "def most_similar_sklearn(array_embedding_sparse, mail_tfidf):\n",
    "    \n",
    "    similarities = cosine_similarity(array_embedding_sparse, mail_tfidf)\n",
    "    if int(round(sorted(similarities[:,0], reverse=True)[0])) ==1:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1][1:]\n",
    "    else:\n",
    "        closest_ids = similarities[:,0].argsort()[::-1]\n",
    "    \n",
    "    return closest_ids\n",
    "\n",
    "def get_sender(query_mid, training):\n",
    "    for row in training.iterrows():\n",
    "        mids = row[1]['mids'].split()\n",
    "        for mid in mids:\n",
    "            if int(mid) == query_mid:\n",
    "                sender = row[1]['sender']\n",
    "                break\n",
    "    return sender\n",
    "\n",
    "def get_n_closest_emails(sender, n, closest_ids, training, training_info, mail_date):\n",
    "    # Get all emails' mids from query sender\n",
    "    all_emails_from_sender_mids = [int(k) for k in training[training['sender']==sender]['mids'].values[0].split()]\n",
    "\n",
    "    # Get emails' index from query sender\n",
    "    training_info_anterior = training_info[training_info['date'] <= mail_date]\n",
    "    all_emails_from_sender_ids = training_info_anterior[training_info_anterior['mid'].isin(all_emails_from_sender_mids)].index.values\n",
    "\n",
    "    # Get the closest emails WRITTEN BY THE SENDER\n",
    "    closest_ids_per_sender = []\n",
    "    for idx in closest_ids:\n",
    "        if idx in all_emails_from_sender_ids:\n",
    "            closest_ids_per_sender.append(idx)\n",
    "        if len(closest_ids_per_sender) == n:\n",
    "            break\n",
    "            \n",
    "    return closest_ids_per_sender\n",
    "\n",
    "def get_10_recipients(closest_ids_per_sender, training_info, closest_emails_dates):\n",
    "    dic_of_recipients = {}\n",
    "    weight = len(closest_ids_per_sender)+1\n",
    "    for idx in closest_ids_per_sender:\n",
    "        recipients = training_info.loc[idx,'recipients'].split()\n",
    "        for recipient in recipients:\n",
    "            if '@' in recipient:\n",
    "                if recipient not in dic_of_recipients.keys():\n",
    "                    dic_of_recipients[recipient] = 0.5*weight*closest_emails_dates['weight_date'][idx]\n",
    "                else:\n",
    "                    dic_of_recipients[recipient] += 0.5*weight*closest_emails_dates['weight_date'][idx]\n",
    "        weight-=1\n",
    "\n",
    "    suggested_10_recipients = heapq.nlargest(10, dic_of_recipients, key=dic_of_recipients.get)\n",
    "    \n",
    "    return suggested_10_recipients\n",
    "\n",
    "def mean_ap(suggested_10_recipients, ground_truth):\n",
    "    MAP = 0\n",
    "    correct_guess = 0\n",
    "    for i, suggestion in enumerate(suggested_10_recipients):\n",
    "        if suggestion in ground_truth:\n",
    "            correct_guess +=1\n",
    "            MAP += float(correct_guess)/(i+1)\n",
    "    MAP = float(MAP)/min(10, len(ground_truth))\n",
    "    return MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n"
     ]
    }
   ],
   "source": [
    "X_train_info.index = range(X_train_info.shape[0])\n",
    "duplicate_test_set_ids = []\n",
    "count=0\n",
    "for mail in X_test_info['body'].values:\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    similarities = cosine_similarity(array_embedding_sparse, mail_tfidf)\n",
    "    \n",
    "    closest_id = similarities[:,0].argsort()[::-1][0]\n",
    "    duplicate_test_set_ids.append(closest_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = True\n",
    "training_info = training_info.sort_values(by='date')\n",
    "\n",
    "if submission:\n",
    "    # submission procedure\n",
    "    X_train_info = training_info\n",
    "    X_test_info = test_info\n",
    "    \n",
    "else:\n",
    "    # test procedure\n",
    "    split_date=datetime(2001, 8, 25)\n",
    "    X_train_info = training_info[training_info.date <= split_date]\n",
    "    #X_test_info = training_info[training_info.date > split_date]\n",
    "    \n",
    "    #Randomize selection of test set:\n",
    "    X_test_info = training_info[training_info.date > split_date]\n",
    "    mask = nprnd.choice(range(X_test_info.shape[0]), size=1000, replace=False)\n",
    "    X_test_info.index = range(X_test_info.shape[0])\n",
    "    X_test_info = X_test_info[X_test_info.index.isin(mask)]\n",
    "\n",
    "    X_train_info = training_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if submission:\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "    array_embedding_sparse = tfidf.fit_transform(np.concatenate((X_train_info['body'].values,X_test_info['body'].values)))\n",
    "    array_embedding_sparse = array_embedding_sparse[:X_train_info.shape[0]]\n",
    "else:\n",
    "    #With porter stemming:\n",
    "    #tfidf = TfidfVectorizer(tokenizer= tokenize, stop_words = stop_words)\n",
    "    #Without stemming:\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_words)\n",
    "    array_embedding_sparse = tfidf.fit_transform(X_train_info['body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "0:04:42.486672\n"
     ]
    }
   ],
   "source": [
    "all_mean_ap = []\n",
    "all_ground_truth = []\n",
    "all_suggestions = []\n",
    "results = pd.DataFrame(columns=['recipients'])\n",
    "results.index.name = 'mid'\n",
    "# number of closest neighbors to collect recipients from:\n",
    "n = 30\n",
    "\n",
    "#re-arrange train index\n",
    "X_train_info.index = range(X_train_info.shape[0])\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "count = 1\n",
    "query_id = 45\n",
    "for query_id in X_test_info.index.values:\n",
    "\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "\n",
    "    mail = X_test_info['body'][query_id]\n",
    "    mail_date = X_test_info['date'][query_id]\n",
    "    query_mid = X_test_info['mid'][query_id]\n",
    "\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    closest_ids = most_similar_sklearn(array_embedding_sparse, mail_tfidf)\n",
    "\n",
    "    # find the sender from the query email\n",
    "    # For the real prediction, replace training by test\n",
    "    if submission:\n",
    "        sender = get_sender(query_mid, test)\n",
    "        #sender = X_test_info[X_test_info.mid == query_mid]['sender'].values[0]\n",
    "        \n",
    "    else:\n",
    "        sender = get_sender(query_mid, training)\n",
    "        #sender = X_train_info[X_train_info.mid == query_mid]['sender'].values[0]\n",
    "\n",
    "    # find the closest emails (written by the sender) to the query one\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, X_train_info, mail_date)\n",
    "\n",
    "    closest_emails_dates = pd.DataFrame(X_train_info['date'][closest_ids_per_sender].sort_values())\n",
    "    closest_emails_dates['weight_date'] = range(1, len(closest_ids_per_sender)+1)\n",
    "\n",
    "    if closest_emails_dates[closest_emails_dates['date']>mail_date].shape[0] > 0:\n",
    "        print query_id\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, X_train_info, closest_emails_dates)\n",
    "\n",
    "    if submission:\n",
    "        string_recipients = ''\n",
    "        for k in suggested_10_recipients:\n",
    "            string_recipients+=k + ' '\n",
    "\n",
    "        results.loc[query_mid, 'recipients'] = string_recipients\n",
    "    else:\n",
    "        ground_truth = X_test_info['recipients'][query_id].split()\n",
    "        all_suggestions.append(suggested_10_recipients)\n",
    "        ground_truth.append(ground_truth)\n",
    "        all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))\n",
    "    #print ground_truth\n",
    "    #print\n",
    "    #print suggested_10_recipients\n",
    "print datetime.now()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30680133393172082"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_mean_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.to_csv('../submission/submission_30_with_weights_0-5_linear_dates_linear.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create some handy structures #                    \n",
    "################################\n",
    "                            \n",
    "# convert training set to dictionary\n",
    "emails_ids_per_sender = {}\n",
    "for index, series in training.iterrows():\n",
    "    row = series.tolist()\n",
    "    sender = row[0]\n",
    "    ids = row[1:][0].split(' ')\n",
    "    emails_ids_per_sender[sender] = ids\n",
    "\n",
    "# save all unique sender names\n",
    "all_senders = emails_ids_per_sender.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "# create address book with frequency information for each user\n",
    "address_books = {}\n",
    "i = 0\n",
    "\n",
    "for sender, ids in emails_ids_per_sender.iteritems():\n",
    "    recs_temp = []\n",
    "    for my_id in ids:\n",
    "        recipients = training_info[training_info['mid']==int(my_id)]['recipients'].tolist()\n",
    "        recipients = recipients[0].split(' ')\n",
    "        # keep only legitimate email addresses\n",
    "        recipients = [rec for rec in recipients if '@' in rec]\n",
    "        recs_temp.append(recipients)\n",
    "    # flatten    \n",
    "    recs_temp = [elt for sublist in recs_temp for elt in sublist]\n",
    "    # compute recipient counts\n",
    "    rec_occ = dict(Counter(recs_temp))\n",
    "    # order by frequency\n",
    "    sorted_rec_occ = sorted(rec_occ.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    # save\n",
    "    address_books[sender] = sorted_rec_occ\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print i\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save all unique recipient names    \n",
    "all_recs = list(set([elt[0] for sublist in address_books.values() for elt in sublist]))\n",
    "\n",
    "# save all unique user names \n",
    "all_users = []\n",
    "all_users.extend(all_senders)\n",
    "all_users.extend(all_recs)\n",
    "all_users = list(set(all_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# baselines #                           \n",
    "#############\n",
    "\n",
    "# will contain email ids, predictions for random baseline, and predictions for frequency baseline\n",
    "predictions_per_sender = {}\n",
    "\n",
    "# number of recipients to predict\n",
    "k = 10\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    name_ids = row.tolist()\n",
    "    sender = name_ids[0]\n",
    "    # get IDs of the emails for which recipient prediction is needed\n",
    "    ids_predict = name_ids[1].split(' ')\n",
    "    ids_predict = [int(my_id) for my_id in ids_predict]\n",
    "    random_preds = []\n",
    "    freq_preds = []\n",
    "    # select k most frequent recipients for the user\n",
    "    k_most = [elt[0] for elt in address_books[sender][:k]]\n",
    "    for id_predict in ids_predict:\n",
    "        # select k users at random\n",
    "        random_preds.append(random.sample(all_users, k))\n",
    "        # for the frequency baseline, the predictions are always the same\n",
    "        freq_preds.append(k_most)\n",
    "    predictions_per_sender[sender] = [ids_predict,random_preds,freq_preds]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "# write predictions in proper format for Kaggle #                           \n",
    "#################################################\n",
    "\n",
    "path_to_results = '../submission/'\n",
    "\n",
    "with open(path_to_results + 'predictions_random.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        random_preds = preds[1]\n",
    "        for index, my_preds in enumerate(random_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')\n",
    "\n",
    "with open(path_to_results + 'predictions_frequency.csv', 'wb') as my_file:\n",
    "    my_file.write('mid,recipients' + '\\n')\n",
    "    for sender, preds in predictions_per_sender.iteritems():\n",
    "        ids = preds[0]\n",
    "        freq_preds = preds[2]\n",
    "        for index, my_preds in enumerate(freq_preds):\n",
    "            my_file.write(str(ids[index]) + ',' + ' '.join(my_preds) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code to create the person_id list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "person_id = {}\n",
    "idx=0\n",
    "for recipient_list in training_info['recipients'].values:\n",
    "    for recipient in recipient_list.split():\n",
    "        if '@' in recipient:\n",
    "            if recipient in person_id.keys():\n",
    "                pass\n",
    "            else:\n",
    "                person_id[recipient] = idx\n",
    "                idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sender in training['sender']:\n",
    "    if sender not in person_id.keys():\n",
    "        print sender\n",
    "        person_id[sender] = idx\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('../data/person_id.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(unicode(json.dumps(person_id, ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    assert len(a) == len(b), 'vectors need to have the same size'\n",
    "    cos_sim = a.dot(b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_n_similar(idx, array_embedding, n):\n",
    "    query_embed = array_embedding[idx]\n",
    "    list_scores = {}\n",
    "    for i in range(idx) + range(idx + 1, len(array_embedding)):\n",
    "        list_scores[i] = cosine_similarity(query_embed, array_embedding[i])\n",
    "\n",
    "    closest_n_idx = heapq.nlargest(n, list_scores, key=list_scores.get)\n",
    "    \n",
    "    return closest_n_idx, list_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First version of the frequentist algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id = 5\n",
    "all_mean_ap = []\n",
    "\n",
    "\n",
    "for query_id in training_info.tail(1000).index.values:\n",
    "\n",
    "    if query_id%100==0:\n",
    "        print query_id\n",
    "    # number of closest neighbors to collect recipients from:\n",
    "    n = 30\n",
    "\n",
    "    # for testing:\n",
    "    mail = training_info['body'].values[query_id]\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    # ground truth for scoring\n",
    "    ground_truth = training_info['recipients'][query_id].split()\n",
    "\n",
    "    # All closest emails (we will select the ones from the sender later)\n",
    "    closest_ids = most_similar_sklearn(array_embedding_sparse, mail_tfidf)\n",
    "\n",
    "    query_mid = training_info['mid'][query_id]\n",
    "\n",
    "    # find the sender from the query email\n",
    "    sender = get_sender(query_mid, training)\n",
    "\n",
    "    # find the closest emails to the query one, written by the sender\n",
    "    closest_ids_per_sender = get_n_closest_emails(sender, n, closest_ids, training, training_info)\n",
    "\n",
    "    # Create dictionnary of all recipient for the 30 most similar emails, and get frequency\n",
    "    # For the moment it is only the brute frequency, maybe we could refine this by adding wheights according to the closseness of the email\n",
    "    suggested_10_recipients = get_10_recipients(closest_ids_per_sender, training_info)\n",
    "\n",
    "    # print\n",
    "    # print suggested_10_recipients\n",
    "    all_mean_ap.append(mean_ap(suggested_10_recipients, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_info.index = range(X_train_info.shape[0])\n",
    "duplicate_test_set_ids = []\n",
    "count=0\n",
    "for mail in X_test_info['body'].values:\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print count\n",
    "    mail_tfidf = tfidf.transform([mail])\n",
    "    similarities = cosine_similarity(array_embedding_sparse, mail_tfidf)\n",
    "    \n",
    "    closest_id = similarities[:,0].argsort()[::-1][0]\n",
    "    duplicate_test_set_ids.append(closest_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
